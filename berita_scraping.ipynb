{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Mengambil berita dari tag: bulutangkis (Kategori: Non-Sepak Bola)\n",
      "\n",
      "🔄 Mengambil berita dari tag: basket (Kategori: Non-Sepak Bola)\n",
      "\n",
      "🔄 Mengambil berita dari tag: voli (Kategori: Non-Sepak Bola)\n",
      "\n",
      "🔄 Mengambil berita dari tag: liga-inggris (Kategori: Liga Inggris)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0'\n",
    "# }\n",
    "\n",
    "# Daftar kategori dan tag URL untuk tiga sumber berita\n",
    "kategori_list = {\n",
    "    'Non-Sepak Bola': ['bulutangkis', 'basket', 'voli'],\n",
    "    'Liga Inggris': ['liga-inggris'],\n",
    "    'Liga Indonesia': ['liga-1'],\n",
    "    'Liga Spanyol': ['la-liga'],\n",
    "    'Liga Italia': ['serie-a'],\n",
    "    'Liputan6': ['liputan6-sepakbola', 'liputan6-olahraga'],\n",
    "    'Kompas': ['kompas-olahraga', 'kompas-bola']\n",
    "}\n",
    "\n",
    "# Daftar sumber berita untuk digunakan dalam scraping\n",
    "sources = {\n",
    "    'Detik': 'https://www.detik.com/tag/',\n",
    "    'Liputan6': 'https://www.liputan6.com/tag/',\n",
    "    'Kompas': 'https://www.kompas.com/tag/'\n",
    "}\n",
    "\n",
    "semua_berita = []\n",
    "nomor = 1\n",
    "\n",
    "# Fungsi untuk mengecek dan memproses artikel\n",
    "def proses_artikel(link, kategori):\n",
    "    try:\n",
    "        r_artikel = requests.get(link, headers=headers)\n",
    "        soup_artikel = BeautifulSoup(r_artikel.text, 'html.parser')\n",
    "\n",
    "        # Ambil isi teks berita\n",
    "        paragraphs = soup_artikel.select('div.detail__body-text p')  # Sesuaikan selector untuk setiap sumber\n",
    "        isi_teks = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        # Ambil tanggal artikel\n",
    "        if 'detik.com' in link:\n",
    "            tanggal_artikel = soup_artikel.select_one('div.date')\n",
    "            format_tanggal = \"%d %b %Y\"\n",
    "        elif 'liputan6.com' in link:\n",
    "            tanggal_artikel = soup_artikel.select_one('div.date_publish')\n",
    "            format_tanggal = \"%d %b %Y\"\n",
    "        elif 'kompas.com' in link:\n",
    "            tanggal_artikel = soup_artikel.select_one('div.date')\n",
    "            format_tanggal = \"%d %b %Y\"\n",
    "\n",
    "        if tanggal_artikel:\n",
    "            try:\n",
    "                tanggal_artikel = datetime.strptime(tanggal_artikel.get_text().strip(), format_tanggal)\n",
    "                if (datetime.now() - tanggal_artikel).days <= 180:  # Pastikan artikel dalam 6 bulan terakhir\n",
    "                    if len(isi_teks.split()) >= 100:  # Pastikan teks lebih dari 100 kata\n",
    "                        return {\n",
    "                            'No': nomor,\n",
    "                            'Text': isi_teks,\n",
    "                            'URL': link,\n",
    "                            'Kategori': kategori\n",
    "                        }\n",
    "            except ValueError:\n",
    "                print(f\"❗ Format tanggal tidak dikenali: {tanggal_artikel.get_text()}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❗ Error mengambil {link}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Mengambil berita dari setiap kategori dan sumber\n",
    "for kategori, tag_list in kategori_list.items():\n",
    "    for tag in tag_list:\n",
    "        print(f\"\\n🔄 Mengambil berita dari tag: {tag} (Kategori: {kategori})\")\n",
    "\n",
    "        # Iterasi untuk mengambil berita dari setiap sumber\n",
    "        for sumber, base_url in sources.items():\n",
    "            halaman = 1\n",
    "            berita_per_kategori = 0\n",
    "            while halaman <= 3 and berita_per_kategori < 25:  # Max 3 halaman, 25 berita per kategori\n",
    "                url = f\"{base_url}{tag}?page={halaman}\"\n",
    "                res = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "                # Ambil semua link artikel\n",
    "                links = soup.select(\"article a\")\n",
    "                for a in links:\n",
    "                    link = a['href']\n",
    "                    # Cek dan proses artikel\n",
    "                    berita = proses_artikel(link, kategori)\n",
    "                    if berita:\n",
    "                        if link not in [b['URL'] for b in semua_berita]:\n",
    "                            semua_berita.append(berita)\n",
    "                            nomor += 1\n",
    "                            berita_per_kategori += 1\n",
    "                            if berita_per_kategori >= 25:\n",
    "                                break\n",
    "                halaman += 1\n",
    "                time.sleep(2)\n",
    "\n",
    "print(f\"\\n✅ Total berita terkumpul: {len(semua_berita)}\")\n",
    "\n",
    "# Simpan ke DataFrame dan ekspor ke CSV\n",
    "df_final = pd.DataFrame(semua_berita)\n",
    "df_final.to_csv(\"berita_olahraga_scraped.csv\", index=False)\n",
    "print(\"\\n📁 Data berhasil disimpan ke 'berita_olahraga_scraped.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
